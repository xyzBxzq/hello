{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3441c7c4-aaad-4605-b1b1-9fc39e98631a",
   "metadata": {},
   "source": [
    "## 2.2 文本分词样例\n",
    "### 使用 Edith Wharton的短篇小说 The Verdict作为分词文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4cb27d-97c3-44d9-8426-89fd73d36398",
   "metadata": {},
   "source": [
    "### 下载该文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "358e490b-9245-49d9-8787-e31696991e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已成功下载到 the-verdict.txt\n"
     ]
    }
   ],
   "source": [
    "# import urllib.request\n",
    "# # url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "# # \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "# # \"the-verdict.txt\")\n",
    "# url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "# file_path = \"the-verdict.txt\"\n",
    "# urllib.request.urlretrieve(url, file_path)\n",
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "# 定义请求头信息\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 现在可以正确使用headers变量了\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # 检查HTTP错误状态码（如404、403）\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"文件已成功下载到 {file_path}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"请求失败：{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bac74c-485e-40ad-920a-e78daf2db90a",
   "metadata": {},
   "source": [
    "### 通过 Python读取短篇小说 The Verdict作为文本样本，并打印出文件的字符总数以及文件的前一百个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52f7f63-7a32-4768-b0e4-2afa05c424b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    print(\"Total number of character:\", len(raw_text))\n",
    "    print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e501d0-be2a-4ba8-9afa-5b37ea7e6b6b",
   "metadata": {},
   "source": [
    "### 进行文本分割的小练习 以简单文本为例，使用re.split按照空白字符分割文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116cb9ee-8262-45b9-973f-38de50653063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa36ad-e75c-4d9a-ad40-233940eab1f9",
   "metadata": {},
   "source": [
    "### 得到的结果是一个包含单个单词、空白字符和标点符号的列表\n",
    "##### ['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a796b-c334-4c03-adcd-adeabae02adc",
   "metadata": {},
   "source": [
    "### 接下来修改正则表达式，使其在空白字符（\\s）、逗号和句号（[,.]）处进行分割："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08f47b99-8f90-4fd0-8ed8-faeb16a74fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf408feb-f316-4101-a089-50933db16aa6",
   "metadata": {},
   "source": [
    "### 可以看到，我们如愿地将单词和标点符号分割成了独立的列表项：\n",
    "###### ['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is',' ', 'a', ' ', 'test', '.', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f675f-2b53-4bbd-8a97-af1aad974ddb",
   "metadata": {},
   "source": [
    "### 一个小问题是列表中仍然包含空白字符。可以通过以下方法安全地删除这些冗余字符：\n",
    "### item.strip() 的作用：该方法会移除字符串前后的所有空白字符（包括空格、换行符 \\n、制表符 \\t 等）。\n",
    "### 如果 item 是纯空白字符串（如 \"\"、\" \"、\"\\n\\t\"），strip() 会返回空字符串 \"\"。\n",
    "### 如果 item 包含非空白内容（如 \" hello \"），strip() 会返回去除首尾空白后的非空字符串（如 \"hello\"）。\n",
    "### [item for item in result if item.strip()] 中，if item.strip() 是过滤条件。\n",
    "### 在 Python 中，空字符串 \"\" 被视为 False，非空字符串被视为 True。因此：\n",
    "### 当 item 是纯空白字符串时，item.strip() 返回 \"\"，条件为 False，该元素会被过滤掉。\n",
    "### 当 item 包含有效内容时，item.strip() 返回非空字符串，条件为 True，该元素会被保留。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29dfc595-d21a-4af8-8f16-df002220e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2505995-1958-48e1-87c0-8a509032cb19",
   "metadata": {},
   "source": [
    "### 去除空白字符后的输出如下所示。\n",
    "###### ['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n",
    "### 移除空白字符可以减轻内存和计算的负担。然而，如果训练的模型需要对文本的精确结构保持敏感，那么保留空白字符就显得尤为重要\n",
    "### 再修改一下，使其能够处理其他类型的标点符号，比如问号、引号，以及短篇小说 The Verdict的前 100个字符中出现的双破折号等特殊字符："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f8e1793-cbbe-4fa6-9aff-c13d266a8f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90cf7e-b8e6-455d-aae0-654b47f069bb",
   "metadata": {},
   "source": [
    "### 修改后的输出如下所示。\n",
    "###### ['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n",
    "![图2-5 分词效果](pic/2-5pic.png)\n",
    "### 现在，我们已经构建了一个简易分词器，让我们将其应用于短篇小说 The Verdict的全文："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28cd5782-f205-428d-9e53-8095d23abf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca6cf4-bb8b-4919-803e-290eb876333e",
   "metadata": {},
   "source": [
    "### 上述打印语句的输出是 4690，这是该文本的词元数量（不包括空白字符）。为了快速查看分词效果，可以打印前 30个词元："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a4f28da-c88c-4c0d-876a-195d1a064f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef668c-ebde-4bf5-acea-e9e72bd61382",
   "metadata": {},
   "source": [
    "### 结果显示，分词器似乎很好地处理了文本，因为所有单词和特殊字符都被整齐地分开了。\n",
    "###### ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
    "### 现在我们已经完成了短篇小说 The Verdict的分词，并将结果存储在名为 preprocessed 的Python变量中。接下来，我们将创建一个包含所有唯一词元的列表，并将它们按照字母顺序排列，以确定词汇表的大小：\n",
    "### 词汇表大小 vocab_size 本质上是文本中不重复的单词总数，通过「去重→排序→计数」三个步骤计算得出。\n",
    "### set(preprocessed)：将 preprocessed 列表（通常是预处理后的文本单词列表）转换为集合（set）。集合的特性是自动去重，因此这一步会保留所有唯一的单词（去除重复出现的单词）。\n",
    "### sorted(...)：对去重后的单词集合进行排序（按字母顺序），最终得到一个包含所有不重复单词的有序列表 all_words，这就是构建的「词汇表」。\n",
    "### len(all_words)：计算词汇表列表的长度，即不重复单词的总数量，这就是 vocab_size（词汇表大小）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "855c3dac-5cc2-4d8a-9d07-3424ac0a23eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc3eb4-2388-490d-803e-bfc3d2d77f4e",
   "metadata": {},
   "source": [
    "### 通过运行上述代码可以确定词汇表的大小为 1130。随后，我们创建词汇表，并打印该词汇表的前 51个条目作为示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a36f6c-374a-4456-9c5a-cf6788997e81",
   "metadata": {},
   "source": [
    "## 2.3 将词元转换为词元ID\n",
    "### 创建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ce31cb7-e963-4547-b93c-c5ee1cdfd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)} # all_words是之前处理得到的有序唯一单词列表，创建词汇表：将每个单词映射到一个整数索引\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab8fb0-ba26-4f99-a7c3-0ca0fabf720e",
   "metadata": {},
   "source": [
    "### 下一个目标是使用这张词汇表将新文本转换为词元 ID\n",
    "![图2-7 词元ID转换](pic/2-7pic.png)\n",
    "### 为了将大语言模型的输出从数值形式转换回文本，还需要一种将词元 ID转换为文本的方法。为此，可以创建逆向词汇表，将词元 ID映射回它们对应的文本词元。\n",
    "## 在Python中实现一个完整的分词器类\n",
    "### 此类包含一个用于将文本分词的encode方法，并通过词汇表将字符串映射到整数，以生成词元 ID。此外，我们还将实现一个 decode 方法，执行从整数到字符串的反向映射，将词元 ID还原回文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5c06978-6be5-4821-813d-a7089a9d1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # 将词汇表作为类属性存储，以便在 encode 方法和 decode 方法中访问\n",
    "        self.int_to_str = {\n",
    "            i:s for s,i in vocab.items() # 遍历这些键值对时，将「键和值互换」，生成新的键值对 (索引, 单词)\n",
    "        } # 创建逆向词汇表将词元 ID 映射回原始文本词元\n",
    "        \n",
    "    def encode(self, text): # 处理输入文本，将其转换为词元ID\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [\n",
    "            self.str_to_int[s] for s in preprocessed # 将单词转化为整数 ID\n",
    "        ]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids): # 将词元ID转换为文本\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) # 将整数 ID 转换为单词/标点 并进行拼接\n",
    "        \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # 移除特定标点符号前的空格\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994a5f6-8fbc-49d6-ac57-b4bd190574d4",
   "metadata": {},
   "source": [
    "### 使用 SimpleTokenizerV1 类的编码过程以及解码过程示例\n",
    "![图2-8 分词器对象编码解码过程](pic/2-8pic.png)\n",
    "### 们创建一个 SimpleTokenizerV1 类的分词器实例对象，并将其应用于短篇小说 The Verdict中的一段文本，试试看分词器的实际效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16b2fd71-22f7-4900-9b0d-a8b3394ae258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab) # 初始化分词器，传入预先定义的词汇表 分词器会自动构建两个映射\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text) # 编码过程\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c636b47-284b-4ad2-85d8-9df4b1207003",
   "metadata": {},
   "source": [
    "### 接下来，试试 decode 方法能否将这些词元 ID转换回文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2dab70fa-630a-4aff-89f7-1e71b7b13cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae9a0f-dd19-414e-9355-6f3e046b3e9c",
   "metadata": {},
   "source": [
    "### 成功实现了一个能够基于训练集对文本进行分词和反分词的分词器。现在，将这个分词器应用于训练集之外的新样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5fc6657-fe2b-4a11-adc2-9377bb359347",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      9\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     10\u001b[39m preprocessed = [\n\u001b[32m     11\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     12\u001b[39m ]\n\u001b[32m     13\u001b[39m ids = [\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;66;03m# 将单词转化为整数 ID\u001b[39;00m\n\u001b[32m     15\u001b[39m ]\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4289cdc-88c1-4ef7-b8d6-0cf5dbe2253c",
   "metadata": {},
   "source": [
    "### 执行上述代码将导致以下错误： KeyError: 'Hello'\n",
    "### 问题在于，“Hello”这一单词并未在短篇小说 The Verdict中出现，因此没有被收录到词汇表中。这一现象凸显了在处理大语言模型时，使用规模更大且更多样化的训练集来扩展词汇表的必要性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14abb98c-9022-4fad-80aa-be38e05abe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(',', 0)\n",
      "('?', 1)\n",
      "('Hello', 2)\n",
      "('do', 3)\n",
      "('like', 4)\n",
      "('tea', 5)\n",
      "('you', 6)\n",
      "[2, 0, 1]\n",
      "Hello,?\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "pre = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text1)\n",
    "pre = [item.strip() for item in pre if item.strip()]\n",
    "all = sorted(set(pre))\n",
    "size = len(all)\n",
    "list = {token:integer for integer, token in enumerate(all)}\n",
    "for i,item in enumerate(list.items()):\n",
    "    print(item)\n",
    "class ST:\n",
    "    def __init__(self, list):\n",
    "        self.str_to_int = list\n",
    "        self.int_to_str = {i:s for s,i in list.items()}\n",
    "    def encode(self, text):\n",
    "        pre = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        pre = [item.strip() for item in pre if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in pre]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "tokenizer = ST(list)\n",
    "text = \"\"\"Hello , ?\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e192bd-8b10-418d-b753-5da9cbd5ae63",
   "metadata": {},
   "source": [
    "### 测试文本 (\"Hello , ?\") 由于只包含「单词 + 标点」，没有多个单词形成的自然空格，导致解码后看起来没有间隔。这是测试用例的特殊性造成的，正常文本（包含多个单词）解码后会保留单词间的空格。\n",
    "## 2.4引入特殊上下文词元\n",
    "### 修改分词器使其在遇到词汇表中不存在的单词时，使用特殊词元<|unk|>代替。\n",
    "![图2-9 特殊词元引入](pic/2-9pic.png)\n",
    "### 还会在不相关的文本之间插入特殊词元，通常会在每个文档或图书的开头插入一个词元，以区分前一个文本源\n",
    "![图2-10 <|endoftext|>词元使用示意](pic/2-10pic.png)\n",
    "## 将特殊词元< unk >和<|endoftext|>添加到词汇表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8202aa-2119-408d-ab54-a1874265ac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a17cc6-ca75-4b7c-a392-cca83581f594",
   "metadata": {},
   "source": [
    "### 根据上述打印语句的输出，更新后的词汇表的大小为 1132（更新前的词汇表的大小为 1130）。\n",
    "### 进行快速验证，打印新的词汇表的最后 5个条目："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926df5b4-536d-4a19-91c8-367957d1d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18410762-89ba-4f2c-a7fe-c68b2152b96d",
   "metadata": {},
   "source": [
    "### 根据代码的输出，可以确认这两个新的特殊词元已经被成功地添加到词汇表中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a6261-29b0-4389-8646-b5eb7a635917",
   "metadata": {},
   "source": [
    "## 实现处理未知单词的文本分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eaa5e2b-6c20-4246-bf2b-bfafd611d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                        else \"<|unk|>\" for item in preprocessed] # 用<unk>词元替换未知单词\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text) # 移除特定标点符号前的空格\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10b5c9-bb45-4d31-8c37-b8e48435ca88",
   "metadata": {},
   "source": [
    "### 使用一个由两个独立且无关的句子拼接承德简单的文本样本进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "516512b9-c813-4cbd-b6c9-039192ace127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2)) #标记文本边界\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51738331-7218-4988-a02e-a4bb41894d4a",
   "metadata": {},
   "source": [
    "### 使用SimpleTokenizerV2 对文本样本进行分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9886d86-3e24-4633-aa60-ec6aded36f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e1323-3804-4832-893f-c1c54e3a887d",
   "metadata": {},
   "source": [
    "### 词元 ID列表包含了一个表示<|endoftext|>分隔符的 1130 词元，以及两个表示未知单词的 1131 词元。\n",
    "### 进行反词元化处理，来快速检查分词器的有效性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2694b802-5dd8-4d07-afb8-304e53f30057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07874002-73b1-44bd-88ac-95a1edace74c",
   "metadata": {},
   "source": [
    "### 通过对比反词元化后的文本与原始输入文本，可以确认训练数据集，即短篇小说 The Verdict，不包含“Hello”和“palace”这两个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6575d0-f300-42c4-9841-e52fbe48370b",
   "metadata": {},
   "source": [
    "## 2.5 一种基于 BPE 概念的更复杂的分词方案\n",
    "### 由于 BPE 的实现相对复杂，该方法使用现有的 Python开源库 tiktoken，它基于 Rust的源代码非常高效地实现了 BPE 算法。\n",
    "### 检查当前安装的 tiktoken 库版本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a608fb13-db97-446b-a53e-393277a962f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e20a5c96-6147-4113-b3ff-8f723b4342d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tiktoken\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f5/6e/5b71578799b72e5bdcef206a214c3ce860d999d579a3b56e74a6c8989ee2/tiktoken-0.11.0-cp312-cp312-win_amd64.whl (884 kB)\n",
      "     ---------------------------------------- 0.0/884.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/884.3 kB ? eta -:--:--\n",
      "     ----------- ---------------------------- 262.1/884.3 kB ? eta -:--:--\n",
      "     --------------------- -------------- 524.3/884.3 kB 840.2 kB/s eta 0:00:01\n",
      "     --------------------------------- ---- 786.4/884.3 kB 1.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ 884.3/884.3 kB 995.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\anna\\.conda\\envs\\myenv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\anna\\.conda\\envs\\myenv\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\anna\\.conda\\envs\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anna\\.conda\\envs\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anna\\.conda\\envs\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anna\\.conda\\envs\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bbbbc4-b2f0-43e3-91fa-54d26a79622a",
   "metadata": {},
   "source": [
    "### 按照以下方式实例化 tiktoken 中的 BPE分词器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "660f6ac1-ec0d-40df-8c42-d94a48cbb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3aec20-517d-4751-b046-e10f6b91269e",
   "metadata": {},
   "source": [
    "### 这个分词器与前面通过 encode 方法实现的 SimpleTokenizerV2 用法相似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffab86f7-f5f5-4ac6-a2ca-04a1096a737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    "    )\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda5594-711c-4f0f-a6c4-369b62a9346b",
   "metadata": {},
   "source": [
    "### 运行上述代码，将打印词元 ID，可以使用 decode 方法将词元 ID转换回文本，同样类似于 SimpleTokenizerV2："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5edf9d8e-0827-4549-9c9c-554f107e31fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3011b6-8662-40e7-84bb-ca120de3816f",
   "metadata": {},
   "source": [
    "### <|endoftext|>词元被分配了一个较大的词元 ID，即 50256。事实上，用于训练GPT-2、GPT-3 和 ChatGPT 中使用的原始模型的 BPE 分词器的词汇总量为 50 257，这意味着<|endoftext|>被分配了最大的词元 ID。\n",
    "### BPE分词器可以正确地编码和解码未知单词，比如“someunknownPlace”。BPE分词器是如何做到在不使用<|unk|>词元的前提下处理任何未知词汇的呢？\n",
    "### BPE 通过将频繁出现的字符合并为子词，再将频繁出现的子词合并为单词，来迭代地构建词汇表。具体来说，BPE首先将所有单个字符（如“a”“b”等）添加到词汇表中。然后，它会将频繁同时出现的字符组合合并为子词。由于初始词汇表包含所有字符（或字节），任何未知词都能被拆分为已有子词（最坏情况拆分为单个字符），不存在 “无法编码” 的情况。\n",
    "## 练习 未知单词的 BPE 使用tiktoken库中的BPE分词器对未知单词“Akwirw ier”进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b520ddf-b22f-4aec-9c69-40a0d677eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "text = (\"Akwirw ier\")\n",
    "tokenizer1 = tiktoken.get_encoding(\"gpt2\")\n",
    "integers = tokenizer1.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer1.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131cde59-0083-4f6d-b3b5-9952a1b8b5c7",
   "metadata": {},
   "source": [
    "## 2.6 使用滑动窗口进行数据采样\n",
    "### 实现一个数据加载器，使用滑动窗口（sliding window）方法从训练数据集中提取输入-目标对。\n",
    "### 首先，使用 BPE分词器对短篇小说 The Verdict的全文进行分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12b08be3-3ff9-4c0f-9b69-5778fa66a067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012e2a0-0240-4f4e-ad09-8fbe58e0dc54",
   "metadata": {},
   "source": [
    "### 从数据集中移除前 50 个词元以便演示，这样做会使得在后续步骤中产生一个更有趣一些的文本段落："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0af1356-d125-48c7-a418-3f191f3b2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a6041-0098-4741-8afd-e1a6d0922014",
   "metadata": {},
   "source": [
    "### 创建下一单词预测任务的输入-目标对的一种简单且直观的方法是定义两个变量：x 和 y。变量 x用于存储输入的词元，变量 y 则用于存储由 x 的每个输入词元右移一个位置所得的目标词元："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4aeca5a3-e9fd-4dec-8ffe-2c62ad4abc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #上下文大小决定了输入中包含多少个词元\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f0de9-6e28-4968-8b79-de8846df1fe2",
   "metadata": {},
   "source": [
    "### 通过处理输入及其相应的目标（将输入右移一个位置），可以创建图 2-12中的下一单词预测任务，如下所示：\n",
    "![图2-12 文本序列预测](pic/2-12pic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f42da41-6123-4560-84f3-6eb0bc353217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i] #取 enc_sample 的前 i 个元素作为 “上下文”。当 i=1 时：context = enc_sample[:1] → 取第 1 个元素（索引 0）作为上下文\n",
    "    desired = enc_sample[i] #取 enc_sample 的第 i 个元素（索引 i）作为 “目标词”。当 i=1 时：desired = enc_sample[1] → 取第 2 个元素作为目标\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5a04f-0324-433e-acef-713daaf1a532",
   "metadata": {},
   "source": [
    "### 箭头（---->）左侧的内容表示大语言模型接收的输入，箭头右侧的词元 ID 则代表大语言模型应该预测的目标词元 ID。为了更直观地展示这一过程，让我们重用前面的代码，但这一次将词元 ID转换回文本形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24835444-6935-42c2-92ea-0b8f86ada113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedfc5e6-8568-4f56-961c-cfdbc365d26f",
   "metadata": {},
   "source": [
    "### 在将词元转化为嵌入向量前，还需要完成最后一项任务：实现一个高效的数据加载器\n",
    "### 这个数据加载器会遍历输入数据集，并将输入和目标以 PyTorch张量的形式返回\n",
    "### 具体来说，我们的目标是返回两个张量：一个是包含大语言模型所见的文本输入的输入张量，另一个是包含大语言模型需要预测的目标词元的目标张量\n",
    "![图2-13 数据加载器目标张量](pic/2-13pic.png)\n",
    "### 为了方便说明，图 2-13中以字符串格式展示了词元，但在实际的代码实现中，我们会直接操作词元 ID，因为 BPE分词器的 encode 方法在一个步骤中同时完成了分词和词元 ID的转换。\n",
    "### 为了实现高效的数据加载器，使用 PyTorch 内置的 Dataset 类和 DataLoader类。\n",
    "## 一个用于批处理输入和目标的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "354d98af-b914-49ad-9f4d-eeb4637fa5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = [] # 存储输入序列\n",
    "        self.target_ids = [] # 存储预测序列\n",
    "        \n",
    "        token_ids = tokenizer.encode(txt) # 对全部文本进行分词 将文本编码为整数 ID序列\n",
    "        # 使用滑动窗口将文本划分为长度为 max_length 的重叠序列\n",
    "        for i in range(0, len(token_ids) - max_length, stride): # 循环范围：从0开始，每次移动stride步，直到剩余长度不足max_length\n",
    "            input_chunk = token_ids[i:i + max_length] # 输入序列：从i开始，取max_length个元素（上下文）\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1] # 目标序列：从i+1开始，取max_length个元素（输入序列的下一个词）\n",
    "            self.input_ids.append(torch.tensor(input_chunk)) # 分别转换为PyTorch张量并存储\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self): # 返回数据集的总行数\n",
    "        return len(self.input_ids)\n",
    "        \n",
    "    def __getitem__(self, idx): # 返回数据集的指定行\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cae94-5714-4726-93e2-f6900f41bf89",
   "metadata": {},
   "source": [
    "### 使用 GPTDatasetV1 通过 PyTorch的 DataLoader 批量加载输入\n",
    "## 用于批量生成输入-目标对的数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d1c518b-ac8e-4100-8143-7fead82908ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "    txt,                  # 原始文本（字符串）\n",
    "    batch_size=4,         # 每个批次的样本数（默认4）\n",
    "    max_length=256,       # 每个样本的序列长度（默认256）\n",
    "    stride=128,           # 滑动窗口的步长（默认128）\n",
    "    shuffle=True,         # 是否打乱样本顺序（默认True）\n",
    "    drop_last=True,       # 是否丢弃最后一个不完整的批次（默认True）\n",
    "    num_workers=0         # 预处理的CPU进程数（默认0，即主进程处理）\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # 初始化分词器\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #创建数据集\n",
    "    dataloader = DataLoader(\n",
    "        dataset,                # 传入自定义数据集\n",
    "        batch_size=batch_size,  # 每个批次包含的样本数\n",
    "        shuffle=shuffle,        # 训练前是否打乱样本顺序（避免模型学习顺序偏见）\n",
    "        drop_last=drop_last,    # 如果 drop_last 为 True 且批次大小小于指定的 batch_size，则会删除最后一批，以防止在训练期间出现损失剧增\n",
    "        num_workers=num_workers #用于预处理的 CPU进程数\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52a1e3-3292-448e-9388-1bc96be69b22",
   "metadata": {},
   "source": [
    "### 用批次大小为 1 的 DataLoader 对上下文长度为 4 的大语言模型进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3459c60c-af8b-4196-b2b1-5630dee4cc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=1, \n",
    "    max_length=4,\n",
    "    stride=1,     # stride=1：滑动窗口每次移动 1 步（样本间重叠度高，用于生成更多样本）\n",
    "    shuffle=False # 不打乱样本顺序（保持原始文本的先后关系，方便调试）\n",
    ")\n",
    "data_iter = iter(dataloader)  # 将dataloader转换为 Python迭代器，以通过 Python 内置的 next()函数获取下一个条目\n",
    "first_batch = next(data_iter) # 通过 next() 函数从迭代器中获取第一个批次的数据\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f586da-8554-4e19-95c6-e162f61a2e6c",
   "metadata": {},
   "source": [
    "### 变量 first_batch 包含两个张量：第一个张量存储输入词元 ID，第二个张量存储目标词元 ID。由于 max_length 被设置为 4，因此这两个张量各自包含 4个词元 ID。\n",
    "## 注意！实际训练大语言模型时，输入大小通常不小于 256。\n",
    "### 为了说明 stride=1 的含义，需要从该数据集中获取另一批数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95415733-3f6a-432e-99c0-2343e4cee51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376064df-4112-4e44-bb0b-42677af896b7",
   "metadata": {},
   "source": [
    "### 可以发现第二批数据的词元 ID相对于第一批整体左移了一个位置。步幅（stride）决定了批次之间输入的位移量，模拟了滑动窗口方法\n",
    "## 练习 具有不同步幅和上下文长度的数据加载器 max_length=2, stride=2 和 max_length=8, stride=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e0980bb-071f-47fe-b9e4-27dc0a9711d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 40, 367]]), tensor([[ 367, 2885]])]\n",
      "[tensor([[2885, 1464]]), tensor([[1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=1, \n",
    "    max_length=2,\n",
    "    stride=2,     # stride=1：滑动窗口每次移动 1 步（样本间重叠度高，用于生成更多样本）\n",
    "    shuffle=False # 不打乱样本顺序（保持原始文本的先后关系，方便调试）\n",
    ")\n",
    "data_iter = iter(dataloader)  # 将dataloader转换为 Python迭代器，以通过 Python 内置的 next()函数获取下一个条目\n",
    "first_batch = next(data_iter) # 通过 next() 函数从迭代器中获取第一个批次的数据\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab7c7f0a-e531-4873-82d3-aaf48b1618d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464, 1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899]])]\n",
      "[tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, \n",
    "    batch_size=1, \n",
    "    max_length=8,\n",
    "    stride=2,     # stride=1：滑动窗口每次移动 1 步（样本间重叠度高，用于生成更多样本）\n",
    "    shuffle=False # 不打乱样本顺序（保持原始文本的先后关系，方便调试）\n",
    ")\n",
    "data_iter = iter(dataloader)  # 将dataloader转换为 Python迭代器，以通过 Python 内置的 next()函数获取下一个条目\n",
    "first_batch = next(data_iter) # 通过 next() 函数从迭代器中获取第一个批次的数据\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e58aa-df05-4a93-af82-58711614c625",
   "metadata": {},
   "source": [
    "### 较小的批次大小会减少训练过程中的内存占用，但同时会导致在模型更新时产生更多的噪声\n",
    "### 如果步幅与输入窗口大小相等，则可以避免批次之间的重叠\n",
    "### 如何以大于 1的批次大小使用数据加载器进行采样：\n",
    "### 此处选择将步幅增加到 4 来充分利用数据集（不会跳过任何一个单词），同时避免不同批次之间的数据重叠，因为过多的重叠可能会增加模型过拟合的风险。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c7db744-3e8f-46b1-9e8b-78cf97e23337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "# 从迭代器中取出第一个批次，拆分为 inputs（输入序列）和 targets（目标序列）\n",
    "inputs, targets = next(data_iter) # 由于 batch_size=8，inputs 和 targets 的形状都是 (8, 4)（8 个样本，每个样本长度 4）\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d8cc0-57df-4215-9fb4-1acf7de166bb",
   "metadata": {},
   "source": [
    "## 2.7 词元 ID转换为嵌入向量的工作原理\n",
    "### 假设有 4个 ID分别为 2、3、5和 1的输入词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e289b2b2-6725-4184-a83b-d35b35dcc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebc5bb-0726-4ae8-b6e6-12733caa14ab",
   "metadata": {},
   "source": [
    "### 假设我们有一张仅包含 6个单词的小型词汇表（而非 BPE分词器中包含 50 257个单词的词汇表），并且想要创建维度为 3的嵌入（GPT-3的嵌入维度是 12 288）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6894810-775d-4378-9545-5c4ba12a4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4ea8c-e49b-4054-9dfb-78557fe0d723",
   "metadata": {},
   "source": [
    "### 利用 vocab_size 和 output_dim 在 PyTorch中实例化一个嵌入层。此外，为了确保结果的可重复性，将随机种子设置为 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f648268b-bf0f-4a14-a851-904af82e6b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e63472-0cc3-4e50-aa92-d386e4cd22f8",
   "metadata": {},
   "source": [
    "### 权重矩阵具有 6行 3列的结构，其中每一行对应词汇表中的一个词元，每一列则对应一个嵌入维度。现在将其应用到一个词元 ID上，以获取嵌入向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f11152b9-27fd-4f69-b810-7f087da3c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afdf80-d0d9-4276-b6b7-76b747935f98",
   "metadata": {},
   "source": [
    "### 词元 ID为 3的嵌入向量与嵌入矩阵中的第 4行完全相同（由于 Python的索引从 0 开始，因此它对应索引为 3 的行）。换言之，嵌入层实质上执行的是一种查找操作，它根据词元 ID从嵌入层的权重矩阵中检索出相应的行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52acb27-c853-4ae6-9fd3-6071e01423b4",
   "metadata": {},
   "source": [
    "### 将这个方法应用到所有 4个输入 ID（torch.tensor([2, 3, 5, 1])）上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c22f20a-792d-4c2c-aca8-5c60679acb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb25d9-d5db-4a27-8dd8-93c6776c5360",
   "metadata": {},
   "source": [
    "### 这个矩阵中的每一行都是从嵌入权重矩阵中查找获得的，如图 2-16所示。\n",
    "![2-16 ](pic/2-16pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642ae16-5967-4c5e-a505-cd62f18ff0a7",
   "metadata": {},
   "source": [
    "### 将对这些嵌入向量进行细微的调整，以编码词元在文本中的位置信息。\n",
    "## 2.8 编码单词位置信息\n",
    "## 创建初始的位置嵌入\n",
    "### 考虑更实际、更实用的嵌入维度，将输入的词元编码为 256 维的向量表示。\n",
    "### 虽然这个维度仍比原始 GPT-3 模型的维度（GPT-3模型的嵌入维度为 12 288）要小，但对实验来说是合理的。\n",
    "### 此外，假设这些词元 ID是由我们之前实现的词汇量为 50 257的 BPE分词器创建的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c1e3bbdc-fd31-48ed-8321-fa952546f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442be319-f6bd-4c40-a39d-c87b87b25c7b",
   "metadata": {},
   "source": [
    "### 使用上述的 token_embedding_layer，当我们从数据加载器中采样数据时，每个批次中的每个词元都将被嵌入为一个 256维的向量。如果设定批次大小为 8，且每个批次包含 4个词元，则结果将是一个 8×4×256的张量。\n",
    "## 实例化 2.6节中的数据加载器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2e431441-1c91-454c-b884-9d2adc8c7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb2f53-4a54-4942-aea6-82dbc3a76191",
   "metadata": {},
   "source": [
    "### 现在，使用嵌入层将这些词元 ID嵌入 256维的向量中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c3f6a497-56a0-4ca4-8cd2-4ffd8c84dd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee0a1a-a74e-4732-ac3a-f3b7d3f700b5",
   "metadata": {},
   "source": [
    "### 该张量的维度为 8×4×256，这意味着每个词元 ID都已被嵌入一个 256维的向量中 \n",
    "### （8：8 个样本（批次大小）。\n",
    "###   4：每个样本的序列长度（4 个词元）。\n",
    "###   256：每个词元的嵌入向量维度。）\n",
    "### 为了获取GPT模型所采用的绝对位置嵌入，只需创建一个维度与 token_embedding_layer相同的嵌入层即可：\n",
    "### pos_embeddings 的输入通常是一个占位符向量 torch.arange(context_length)，它包含一个从 0开始递增，直至最大输入长度减 1的数值序列。context_length 是一个变量，表示模型支持的输入块的最大长度。我们将其设置为与输入文本的最大长度一致。在实际情况中，输入文本的长度可能会超出模型支持的块大小，这时需要截断文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c0996ee3-5004-491c-bc3c-ea22d361369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length # 将位置嵌入的长度与输入序列的最大长度（max_length）绑定，确保每个位置（如序列中的第 0 个、第 1 个元素）都有对应的位置嵌入。\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim) # 同样使用嵌入层类，但此处不是映射 “词元 ID”，而是映射 “位置索引”。\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length)) # 生成一个从 0 到context_length-1的整数序列，表示所有位置的索引\n",
    "print(pos_embeddings.shape) # pos_embeddings 是一个形状为 (context_length, output_dim) 的张量，包含所有位置的嵌入向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b773797-8b28-4e89-bf2c-c698453f75ae",
   "metadata": {},
   "source": [
    "### 位置嵌入是序列级别的特征，与批次无关。无论批次大小是多少（8、16 等），同一位置（如位置 0）的嵌入向量是固定的，不需要为每个样本单独创建位置嵌入。\n",
    "### 在实际使用时，会通过「广播机制」将 pos_embeddings（形状 [4, 256]）与词元嵌入（形状 [8, 4, 256]）相加，自动适配批次维度，得到 [8, 4, 256] 的最终输入。\n",
    "### 将这些向量直接添加到词元嵌入中，即将词元嵌入（token_embeddings）与位置嵌入（pos_embeddings）相加，得到同时同时包含词元语义信息和位置信息的最终输入嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "77831f70-fcc7-48cf-b305-c0e1d84bdab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696204f0-7266-40cf-959c-37e0902a3987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
